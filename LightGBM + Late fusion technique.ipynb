{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1aye_2nDLmRZpUGaPKmlAu1gK1bKX9G0J","authorship_tag":"ABX9TyNNVc3wOCtsPUIFKP1/pbHd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import torch\n","import transformers\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import f1_score, precision_score, recall_score\n","import lightgbm as lgb\n","import joblib"],"metadata":{"id":"bYtKQFlqaxZe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# CharBERT Model for Embedding Extraction\n","#Didnot used this embedding\n","class CharBERTEmbedding:\n","    def __init__(self, model_name='bert-base-uncased'):\n","        \"\"\"\n","        Initialize CharBERT model for embedding extraction\n","        \"\"\"\n","        self.tokenizer = transformers.BertTokenizer.from_pretrained(model_name)\n","        self.model = transformers.BertModel.from_pretrained(model_name)\n","        self.model.eval()  # Set to evaluation mode\n","\n","    def extract_embeddings(self, texts, max_length=512):\n","        \"\"\"\n","        Extract embeddings using CharBERT\n","        \"\"\"\n","        # Tokenize and get embeddings\n","        embeddings = []\n","        for text in texts:\n","            # Tokenize and prepare input\n","            inputs = self.tokenizer(\n","                text,\n","                return_tensors='pt',\n","                max_length=max_length,\n","                truncation=True,\n","                padding='max_length'\n","            )\n","\n","            # Extract embeddings\n","            with torch.no_grad():\n","                outputs = self.model(**inputs)\n","                # Use [CLS] token embedding (first token)\n","                embedding = outputs.last_hidden_state[:, 0, :].numpy()\n","                embeddings.append(embedding.flatten())\n","\n","        return np.array(embeddings)"],"metadata":{"id":"P0RNsaLua2x8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Late Fusion Classifier\n","class LateFusionClassifier:\n","    def __init__(self, charbert_embedding, tfidf_vectorizer):\n","        \"\"\"\n","        Initialize Late Fusion Classifier\n","        \"\"\"\n","        self.charbert_embedding = charbert_embedding\n","        self.tfidf_vectorizer = tfidf_vectorizer\n","        self.models = []\n","\n","    def prepare_features(self, X_content):\n","        \"\"\"\n","        Prepare features by combining CharBERT and TF-IDF\n","        \"\"\"\n","        # Extract CharBERT embeddings\n","        charbert_features = self.charbert_embedding.extract_embeddings(X_content)\n","\n","        # Extract TF-IDF features\n","        tfidf_features = self.tfidf_vectorizer.transform(X_content).toarray()\n","\n","        # Concatenate features\n","        combined_features = np.hstack([charbert_features, tfidf_features])\n","\n","        return combined_features\n","\n","    def train(self, X_content, y, label_names):\n","        \"\"\"\n","        Train late fusion models for each label\n","        \"\"\"\n","        # Prepare combined features\n","        X_combined = self.prepare_features(X_content)\n","\n","        # Train models for each label\n","        self.models = []\n","        cv_scores = []\n","\n","        for i in range(y.shape[1]):\n","            # LightGBM parameters\n","            params = {\n","                'objective': 'binary',\n","                'metric': 'auc',\n","                'boosting_type': 'gbdt',\n","                'num_leaves': 31,\n","                'learning_rate': 0.05,\n","                'feature_fraction': 0.9\n","            }\n","\n","            # Train model for this label\n","            model = lgb.LGBMClassifier(**params)\n","            model.fit(X_combined, y[:, i])\n","\n","            self.models.append(model)\n","\n","        return self\n","\n","    def predict(self, X_content, threshold=0.5):\n","        \"\"\"\n","        Make predictions using late fusion\n","        \"\"\"\n","        # Prepare combined features\n","        X_combined = self.prepare_features(X_content)\n","\n","        # Predict for each label\n","        predictions = []\n","        for model in self.models:\n","            label_preds = model.predict_proba(X_combined)[:, 1]\n","            predictions.append((label_preds > threshold).astype(int))\n","\n","        return np.array(predictions).T"],"metadata":{"id":"6re_0mTpa_Kq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Preprocessing and Utility Functions (similar to previous implementation)\n","def parse_techniques(x):\n","    technique_mapping = {\n","        'CharBERT + LateFusion': 'charbert_latefusion',\n","        # Add other mappings as needed\n","    }\n","\n","    if isinstance(x, str):\n","        x = x.strip('[]')\n","        parsed_techs = [tech.strip().strip(\"'\\\"\") for tech in x.split(',') if tech.strip()]\n","    elif isinstance(x, list):\n","        parsed_techs = x\n","    else:\n","        parsed_techs = []\n","\n","    mapped_techs = [\n","        technique_mapping.get(tech.strip(), tech.lower().replace(' ', '_'))\n","        for tech in parsed_techs\n","    ]\n","\n","    return mapped_techs if mapped_techs else ['no_techniques']\n","\n","def evaluate_multilabel(y_true, y_pred, label_names):\n","    \"\"\"\n","    Comprehensive multi-label evaluation\n","    \"\"\"\n","    print(\"\\nOverall Metrics:\")\n","    print(\"-\" * 50)\n","    print(f\"Micro F1 Score: {f1_score(y_true, y_pred, average='micro'):.4f}\")\n","    print(f\"Macro F1 Score: {f1_score(y_true, y_pred, average='macro'):.4f}\")\n","\n","    print(\"\\nDetailed Label Metrics:\")\n","    print(\"-\" * 50)\n","    for i, label in enumerate(label_names):\n","        label_true = y_true[:, i]\n","        label_pred = y_pred[:, i]\n","\n","        print(f\"\\nMetrics for label '{label}':\")\n","        print(f\"F1 Score: {f1_score(label_true, label_pred, average='binary'):.4f}\")"],"metadata":{"id":"ZU4Qz403bFc8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def main():\n","    # Load data\n","    train_df = df  # Assuming df is your training dataframe\n","\n","    # Preprocess techniques\n","    train_df['techniques'] = train_df['techniques'].apply(parse_techniques)\n","\n","    # Prepare TF-IDF Vectorizer\n","    from sklearn.feature_extraction.text import TfidfVectorizer\n","    from sklearn.preprocessing import MultiLabelBinarizer\n","\n","    tfidf_vectorizer = TfidfVectorizer(\n","        max_features=5000,\n","        stop_words='english',\n","        ngram_range=(1, 2)\n","    )\n","\n","    # Prepare Multi-Label Binarizer\n","    mlb = MultiLabelBinarizer()\n","    y = mlb.fit_transform(train_df['techniques'])\n","\n","    # Initialize CharBERT Embedding\n","    charbert_embedding = CharBERTEmbedding()\n","\n","    # Split data\n","    X_train, X_val, y_train, y_val = train_test_split(\n","        train_df['content'], y, test_size=0.2, random_state=42\n","    )\n","\n","    # Prepare TF-IDF\n","    tfidf_vectorizer.fit(train_df['content'])\n","\n","    # Train Late Fusion Classifier\n","    late_fusion_classifier = LateFusionClassifier(\n","        charbert_embedding,\n","        tfidf_vectorizer\n","    )\n","    late_fusion_classifier.train(X_train, y_train, mlb.classes_)\n","\n","    # Predict and Evaluate\n","    y_val_pred = late_fusion_classifier.predict(X_val)\n","    evaluate_multilabel(y_val, y_val_pred, mlb.classes_)\n","\n","    # Optional: Save models\n","    joblib.dump(late_fusion_classifier, 'late_fusion_classifier.joblib')\n","    joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.joblib')\n","    joblib.dump(mlb, 'multilabel_binarizer.joblib')\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"fT3YWVCBbKBy"},"execution_count":null,"outputs":[]}]}