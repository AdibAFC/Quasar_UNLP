{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1aye_2nDLmRZpUGaPKmlAu1gK1bKX9G0J","authorship_tag":"ABX9TyMJ7CuSBYvj+B9LPXVTmFc+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install torch\n","!pip install transformers\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"KgRUq2Re-pCi","executionInfo":{"status":"ok","timestamp":1743188847286,"user_tz":-360,"elapsed":10480,"user":{"displayName":"MD Sagor Chowdhury","userId":"18345468425811967055"}},"outputId":"f22953cd-7e69-44aa-b67e-a8b93525d074"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n","Requirement already satisfied: lightning in /usr/local/lib/python3.11/dist-packages (2.5.1)\n","Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.11/dist-packages (from lightning) (6.0.2)\n","Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2025.3.0)\n","Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (0.14.2)\n","Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (24.2)\n","Requirement already satisfied: torch<4.0,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (2.6.0+cu124)\n","Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (1.7.0)\n","Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (4.67.1)\n","Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (4.12.2)\n","Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.11/dist-packages (from lightning) (2.5.1)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.11.14)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (75.1.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.18.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<4.0,>=2.1.0->lightning) (1.3.0)\n","Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics<3.0,>=0.7.0->lightning) (2.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.2.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (0.3.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.18.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (3.0.2)\n","Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (3.10)\n"]}]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim import AdamW\n","from transformers import AutoTokenizer, AutoModel\n","import numpy as np\n","import ast\n","import matplotlib.pyplot as plt\n","from torch.cuda.amp import autocast, GradScaler\n","import pandas as pd"],"metadata":{"id":"3cmlxb-gAC4e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the datasets\n","combined_df = pd.read_csv('/content/drive/MyDrive/Sharedtask/combined_augmented_dataset_with_dev.csv')\n","train_df = combined_df.copy()\n","test_df = pd.read_csv('/content/drive/MyDrive/Sharedtask/test.csv')  # Replace with your test file path\n","\n","# Preprocess techniques\n","train_df['techniques'] = train_df['techniques'].apply(ast.literal_eval)"],"metadata":{"id":"CUhykPHwV3qj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define techniques columns\n","all_techniques = ['straw_man', 'appeal_to_fear', 'fud', 'bandwagon',\n","                  'whataboutism', 'loaded_language',\n","                  'glittering_generalities', 'euphoria',\n","                  'cherry_picking', 'cliche']\n","\n","# Create label matrix for training data\n","label_matrix = pd.DataFrame(0, index=train_df.index, columns=all_techniques)\n","for idx, techniques in enumerate(train_df['techniques']):\n","    for technique in techniques:\n","        if technique in all_techniques:\n","            label_matrix.at[idx, technique] = 1\n","train_df = pd.concat([train_df, label_matrix], axis=1)"],"metadata":{"id":"yl22byS-V7ax"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tokenization\n","model_name = \"Alibaba-NLP/gte-multilingual-reranker-base\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","def tokenize_data(df, tokenizer, max_length=128, is_test=False):\n","    print(\"Tokenizing data...\")\n","    encodings = tokenizer(\n","        df['content'].tolist(),\n","        truncation=True,\n","        padding=True,\n","        max_length=max_length,\n","        return_tensors='pt'\n","    )\n","    if is_test:\n","        return encodings\n","    labels = torch.tensor(df[all_techniques].values, dtype=torch.float)\n","    return encodings, labels\n","\n","# Tokenize training and test data\n","train_encodings, train_labels = tokenize_data(train_df, tokenizer)\n","test_encodings = tokenize_data(test_df, tokenizer, is_test=True)"],"metadata":{"id":"b0N6IL9BV_hT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dataset Class\n","class TextDataset(Dataset):\n","    def __init__(self, encodings, labels=None):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):  # Fixed method name\n","        item = {key: val[idx] for key, val in self.encodings.items()}\n","        if self.labels is not None:\n","            item['labels'] = self.labels[idx]\n","        return item\n","\n","    def __len__(self):  # Fixed method name\n","        return len(self.encodings['input_ids'])\n","\n","# Create datasets\n","train_dataset = TextDataset(train_encodings, train_labels)\n","test_dataset = TextDataset(test_encodings)"],"metadata":{"id":"qj0vGQ4SWCiZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Model Definition\n","class MultiLabelClassifier(nn.Module):\n","    def __init__(self, transformer_model_name, num_labels, hidden_dim=256, lstm_layers=2):\n","        super(MultiLabelClassifier, self).__init__()\n","        self.transformer = AutoModel.from_pretrained(transformer_model_name)\n","        transformer_hidden_dim = self.transformer.config.hidden_size\n","\n","        # BiLSTM Layer\n","        self.bilstm = nn.LSTM(input_size=transformer_hidden_dim,\n","                              hidden_size=hidden_dim,\n","                              num_layers=lstm_layers,\n","                              batch_first=True,\n","                              bidirectional=True)\n","\n","        # Classifier\n","        self.classifier = nn.Linear(hidden_dim * 2, num_labels)  # *2 for bidirectional\n","\n","        # Dropout for regularization\n","        self.dropout = nn.Dropout(0.3)\n","\n","    def forward(self, input_ids, attention_mask, labels=None):\n","        transformer_outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n","        sequence_output = transformer_outputs.last_hidden_state  # (batch_size, seq_len, hidden_size)\n","\n","        # Pass through BiLSTM\n","        lstm_output, _ = self.bilstm(sequence_output)  # (batch_size, seq_len, hidden_dim*2)\n","\n","        # Use the last hidden state of the BiLSTM for classification\n","        lstm_last_output = lstm_output[:, -1, :]  # (batch_size, hidden_dim*2)\n","\n","        # Apply dropout\n","        lstm_last_output = self.dropout(lstm_last_output)\n","\n","        # Get logits\n","        logits = self.classifier(lstm_last_output)\n","\n","        # Compute loss if labels are provided\n","        loss = None\n","        if labels is not None:\n","            loss_fn = nn.BCEWithLogitsLoss()\n","            loss = loss_fn(logits, labels)\n","\n","        return loss, logits"],"metadata":{"id":"7nHkWlglWGTZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training Setup\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = MultiLabelClassifier(\n","    transformer_model_name=model_name,\n","    num_labels=len(all_techniques),\n","    hidden_dim=256,\n","    lstm_layers=2\n",")\n","model.to(device)\n","\n","# Optimizer and Learning Rate\n","optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n","scaler = GradScaler()\n","\n","# Training Parameters\n","batch_size = 8  # Reduced from 16 to manage memory\n","num_epochs = 100\n","patience = 2\n","best_loss = float('inf')\n","patience_counter = 0\n","accumulation_steps = 2  # Gradient accumulation to simulate larger batch size\n","\n","# DataLoaders\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, pin_memory=True)\n","\n","# Training Loop with Early Stopping\n","for epoch in range(num_epochs):\n","    print(f\"Epoch {epoch+1}/{num_epochs} started\")\n","    model.train()\n","    total_loss = 0\n","    optimizer.zero_grad()  # Zero gradients at the start\n","\n","    for batch_idx, batch in enumerate(train_loader):\n","        input_ids = batch['input_ids'].to(device, non_blocking=True)\n","        attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n","        labels = batch['labels'].to(device, non_blocking=True)\n","\n","        with autocast():  # Mixed precision to reduce memory usage\n","            loss, logits = model(input_ids, attention_mask, labels)\n","            loss = loss / accumulation_steps  # Scale loss for accumulation\n","\n","        scaler.scale(loss).backward()\n","\n","        if (batch_idx + 1) % accumulation_steps == 0:\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","\n","        total_loss += loss.item() * accumulation_steps  # Adjust for accumulation\n","\n","        if batch_idx % 10 == 0:  # Print every 10 batches\n","            print(f\"Batch {batch_idx}/{len(train_loader)} - Loss: {loss.item() * accumulation_steps:.4f}\")\n","\n","        # Clear memory\n","        del input_ids, attention_mask, labels, loss, logits\n","        torch.cuda.empty_cache()\n","\n","    # Calculate average loss\n","    avg_loss = total_loss / len(train_loader)\n","    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n","\n","    # Early stopping check\n","    if avg_loss < best_loss:\n","        best_loss = avg_loss\n","        patience_counter = 0\n","        torch.save(model.state_dict(), 'best_model.pt')  # Save best model\n","    else:\n","        patience_counter += 1\n","        print(f\"Patience counter: {patience_counter}/{patience}\")\n","\n","    if patience_counter >= patience:\n","        print(f\"Early stopping triggered after epoch {epoch+1}\")\n","        break\n"],"metadata":{"id":"c74h-l6DWPbC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load best model for prediction\n","model.load_state_dict(torch.load('best_model.pt'))\n","model.eval()\n","predictions = []\n","\n","with torch.no_grad():\n","    for batch in test_loader:\n","        input_ids = batch['input_ids'].to(device, non_blocking=True)\n","        attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n","        with autocast():  # Mixed precision for inference\n","            _, logits = model(input_ids, attention_mask)\n","        probs = torch.sigmoid(logits)\n","        preds = (probs > 0.5).float()\n","        predictions.extend(preds.cpu().numpy())\n","\n","        # Clear memory\n","        del input_ids, attention_mask, logits, probs, preds\n","        torch.cuda.empty_cache()\n","\n","# Create Submission DataFrame\n","submission_df = pd.DataFrame(columns=['id'] + all_techniques)\n","submission_df['id'] = test_df['id']\n","\n","for technique in all_techniques:\n","    submission_df[technique] = [pred[all_techniques.index(technique)] for pred in predictions]\n","\n","# Save Submission\n","submission_df.to_csv('submission.csv', index=False)\n","print(\"Submission file created successfully!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"klZeheQKFALr","outputId":"f9371c26-a8b5-4802-df65-4278b5fe888d","collapsed":true},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Tokenizing data...\n","Tokenizing data...\n","The repository for Alibaba-NLP/gte-multilingual-reranker-base contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/Alibaba-NLP/gte-multilingual-reranker-base.\n","You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n","\n","Do you wish to run the custom code? [y/N] y\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-27-84149d87737c>:131: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler()\n","<ipython-input-27-84149d87737c>:157: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast():  # Mixed precision to reduce memory usage\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/100 started\n","Batch 0/1952 - Loss: 0.6987\n","Batch 10/1952 - Loss: 0.6847\n","Batch 20/1952 - Loss: 0.6662\n","Batch 30/1952 - Loss: 0.6352\n","Batch 40/1952 - Loss: 0.6128\n","Batch 50/1952 - Loss: 0.5789\n","Batch 60/1952 - Loss: 0.5683\n","Batch 70/1952 - Loss: 0.5363\n","Batch 80/1952 - Loss: 0.5420\n","Batch 90/1952 - Loss: 0.5278\n","Batch 100/1952 - Loss: 0.4808\n","Batch 110/1952 - Loss: 0.4358\n","Batch 120/1952 - Loss: 0.4840\n","Batch 130/1952 - Loss: 0.4220\n","Batch 140/1952 - Loss: 0.3979\n","Batch 150/1952 - Loss: 0.3691\n","Batch 160/1952 - Loss: 0.4060\n","Batch 170/1952 - Loss: 0.4351\n","Batch 180/1952 - Loss: 0.3204\n","Batch 190/1952 - Loss: 0.3049\n","Batch 200/1952 - Loss: 0.3567\n","Batch 210/1952 - Loss: 0.3681\n","Batch 220/1952 - Loss: 0.4328\n","Batch 230/1952 - Loss: 0.4574\n","Batch 240/1952 - Loss: 0.3497\n","Batch 250/1952 - Loss: 0.4491\n","Batch 260/1952 - Loss: 0.3152\n","Batch 270/1952 - Loss: 0.4779\n","Batch 280/1952 - Loss: 0.4115\n","Batch 290/1952 - Loss: 0.3580\n","Batch 300/1952 - Loss: 0.3936\n","Batch 310/1952 - Loss: 0.2841\n","Batch 320/1952 - Loss: 0.3048\n","Batch 330/1952 - Loss: 0.3126\n","Batch 340/1952 - Loss: 0.3576\n","Batch 350/1952 - Loss: 0.3493\n","Batch 360/1952 - Loss: 0.3541\n","Batch 370/1952 - Loss: 0.2572\n","Batch 380/1952 - Loss: 0.3344\n","Batch 390/1952 - Loss: 0.3138\n","Batch 400/1952 - Loss: 0.3684\n","Batch 410/1952 - Loss: 0.4150\n","Batch 420/1952 - Loss: 0.3443\n","Batch 430/1952 - Loss: 0.3576\n","Batch 440/1952 - Loss: 0.3184\n","Batch 450/1952 - Loss: 0.4145\n","Batch 460/1952 - Loss: 0.2819\n","Batch 470/1952 - Loss: 0.2959\n","Batch 480/1952 - Loss: 0.3367\n","Batch 490/1952 - Loss: 0.3221\n","Batch 500/1952 - Loss: 0.4311\n","Batch 510/1952 - Loss: 0.2966\n","Batch 520/1952 - Loss: 0.2824\n","Batch 530/1952 - Loss: 0.2678\n","Batch 540/1952 - Loss: 0.2708\n","Batch 550/1952 - Loss: 0.2724\n","Batch 560/1952 - Loss: 0.2934\n","Batch 570/1952 - Loss: 0.3668\n","Batch 580/1952 - Loss: 0.2219\n","Batch 590/1952 - Loss: 0.3004\n","Batch 600/1952 - Loss: 0.3816\n","Batch 610/1952 - Loss: 0.3036\n","Batch 620/1952 - Loss: 0.3185\n","Batch 630/1952 - Loss: 0.3266\n","Batch 640/1952 - Loss: 0.3426\n","Batch 650/1952 - Loss: 0.3004\n","Batch 660/1952 - Loss: 0.3750\n","Batch 670/1952 - Loss: 0.2403\n","Batch 680/1952 - Loss: 0.2782\n","Batch 690/1952 - Loss: 0.2949\n","Batch 700/1952 - Loss: 0.2599\n","Batch 710/1952 - Loss: 0.3042\n","Batch 720/1952 - Loss: 0.2430\n","Batch 730/1952 - Loss: 0.4425\n","Batch 740/1952 - Loss: 0.2047\n","Batch 750/1952 - Loss: 0.3463\n","Batch 760/1952 - Loss: 0.2039\n","Batch 770/1952 - Loss: 0.3059\n","Batch 780/1952 - Loss: 0.4116\n","Batch 790/1952 - Loss: 0.3461\n","Batch 800/1952 - Loss: 0.2785\n","Batch 810/1952 - Loss: 0.4123\n","Batch 820/1952 - Loss: 0.2792\n","Batch 830/1952 - Loss: 0.3806\n","Batch 840/1952 - Loss: 0.3179\n","Batch 850/1952 - Loss: 0.3630\n","Batch 860/1952 - Loss: 0.2855\n","Batch 870/1952 - Loss: 0.3777\n","Batch 880/1952 - Loss: 0.2480\n","Batch 890/1952 - Loss: 0.3097\n","Batch 900/1952 - Loss: 0.3437\n","Batch 910/1952 - Loss: 0.2631\n","Batch 920/1952 - Loss: 0.2684\n","Batch 930/1952 - Loss: 0.1936\n","Batch 940/1952 - Loss: 0.2809\n","Batch 950/1952 - Loss: 0.2367\n","Batch 960/1952 - Loss: 0.2952\n","Batch 970/1952 - Loss: 0.1988\n","Batch 980/1952 - Loss: 0.2763\n","Batch 990/1952 - Loss: 0.3530\n","Batch 1000/1952 - Loss: 0.3075\n","Batch 1010/1952 - Loss: 0.2310\n","Batch 1020/1952 - Loss: 0.3612\n","Batch 1030/1952 - Loss: 0.4244\n","Batch 1040/1952 - Loss: 0.3276\n","Batch 1050/1952 - Loss: 0.1915\n","Batch 1060/1952 - Loss: 0.4880\n","Batch 1070/1952 - Loss: 0.3215\n","Batch 1080/1952 - Loss: 0.3253\n","Batch 1090/1952 - Loss: 0.2150\n","Batch 1100/1952 - Loss: 0.2302\n","Batch 1110/1952 - Loss: 0.2032\n","Batch 1120/1952 - Loss: 0.1355\n","Batch 1130/1952 - Loss: 0.2781\n","Batch 1140/1952 - Loss: 0.2492\n","Batch 1150/1952 - Loss: 0.4176\n","Batch 1160/1952 - Loss: 0.2364\n","Batch 1170/1952 - Loss: 0.2814\n","Batch 1180/1952 - Loss: 0.2531\n","Batch 1190/1952 - Loss: 0.3550\n","Batch 1200/1952 - Loss: 0.2669\n","Batch 1210/1952 - Loss: 0.2211\n","Batch 1220/1952 - Loss: 0.2003\n","Batch 1230/1952 - Loss: 0.3042\n","Batch 1240/1952 - Loss: 0.3626\n","Batch 1250/1952 - Loss: 0.2501\n","Batch 1260/1952 - Loss: 0.2675\n","Batch 1270/1952 - Loss: 0.2823\n","Batch 1280/1952 - Loss: 0.3468\n","Batch 1290/1952 - Loss: 0.2653\n","Batch 1300/1952 - Loss: 0.3200\n","Batch 1310/1952 - Loss: 0.2116\n","Batch 1320/1952 - Loss: 0.3907\n","Batch 1330/1952 - Loss: 0.3462\n","Batch 1340/1952 - Loss: 0.2335\n","Batch 1350/1952 - Loss: 0.2743\n","Batch 1360/1952 - Loss: 0.2790\n","Batch 1370/1952 - Loss: 0.2949\n","Batch 1380/1952 - Loss: 0.2359\n","Batch 1390/1952 - Loss: 0.3098\n","Batch 1400/1952 - Loss: 0.2613\n","Batch 1410/1952 - Loss: 0.4006\n","Batch 1420/1952 - Loss: 0.2868\n","Batch 1430/1952 - Loss: 0.1781\n","Batch 1440/1952 - Loss: 0.2051\n","Batch 1450/1952 - Loss: 0.2430\n","Batch 1460/1952 - Loss: 0.3804\n","Batch 1470/1952 - Loss: 0.1939\n","Batch 1480/1952 - Loss: 0.2669\n","Batch 1490/1952 - Loss: 0.3292\n","Batch 1500/1952 - Loss: 0.3199\n","Batch 1510/1952 - Loss: 0.3911\n","Batch 1520/1952 - Loss: 0.2146\n","Batch 1530/1952 - Loss: 0.2888\n","Batch 1540/1952 - Loss: 0.5341\n","Batch 1550/1952 - Loss: 0.2497\n","Batch 1560/1952 - Loss: 0.2162\n","Batch 1570/1952 - Loss: 0.2924\n","Batch 1580/1952 - Loss: 0.1438\n","Batch 1590/1952 - Loss: 0.2328\n","Batch 1600/1952 - Loss: 0.2163\n","Batch 1610/1952 - Loss: 0.2634\n","Batch 1620/1952 - Loss: 0.2437\n","Batch 1630/1952 - Loss: 0.3268\n","Batch 1640/1952 - Loss: 0.2838\n","Batch 1650/1952 - Loss: 0.3187\n","Batch 1660/1952 - Loss: 0.1933\n","Batch 1670/1952 - Loss: 0.1730\n","Batch 1680/1952 - Loss: 0.2077\n","Batch 1690/1952 - Loss: 0.2778\n","Batch 1700/1952 - Loss: 0.3257\n","Batch 1710/1952 - Loss: 0.2019\n","Batch 1720/1952 - Loss: 0.2360\n","Batch 1730/1952 - Loss: 0.2421\n","Batch 1740/1952 - Loss: 0.3205\n","Batch 1750/1952 - Loss: 0.2905\n","Batch 1760/1952 - Loss: 0.2143\n","Batch 1770/1952 - Loss: 0.2351\n","Batch 1780/1952 - Loss: 0.3592\n","Batch 1790/1952 - Loss: 0.2275\n","Batch 1800/1952 - Loss: 0.3112\n","Batch 1810/1952 - Loss: 0.2014\n","Batch 1820/1952 - Loss: 0.3612\n","Batch 1830/1952 - Loss: 0.2293\n","Batch 1840/1952 - Loss: 0.4000\n","Batch 1850/1952 - Loss: 0.2465\n","Batch 1860/1952 - Loss: 0.2779\n","Batch 1870/1952 - Loss: 0.2132\n","Batch 1880/1952 - Loss: 0.2533\n","Batch 1890/1952 - Loss: 0.2758\n","Batch 1900/1952 - Loss: 0.2043\n","Batch 1910/1952 - Loss: 0.3258\n","Batch 1920/1952 - Loss: 0.2132\n","Batch 1930/1952 - Loss: 0.3840\n","Batch 1940/1952 - Loss: 0.2139\n","Batch 1950/1952 - Loss: 0.1738\n","Epoch 1/100, Loss: 0.3220\n","Epoch 2/100 started\n","Batch 0/1952 - Loss: 0.2425\n","Batch 10/1952 - Loss: 0.2140\n","Batch 20/1952 - Loss: 0.1821\n","Batch 30/1952 - Loss: 0.1911\n","Batch 40/1952 - Loss: 0.2656\n","Batch 50/1952 - Loss: 0.2407\n","Batch 60/1952 - Loss: 0.2486\n","Batch 70/1952 - Loss: 0.1649\n","Batch 80/1952 - Loss: 0.2660\n","Batch 90/1952 - Loss: 0.2525\n","Batch 100/1952 - Loss: 0.2113\n","Batch 110/1952 - Loss: 0.2597\n","Batch 120/1952 - Loss: 0.3202\n","Batch 130/1952 - Loss: 0.2319\n","Batch 140/1952 - Loss: 0.3031\n","Batch 150/1952 - Loss: 0.3017\n","Batch 160/1952 - Loss: 0.2161\n","Batch 170/1952 - Loss: 0.2566\n","Batch 180/1952 - Loss: 0.2560\n","Batch 190/1952 - Loss: 0.2829\n","Batch 200/1952 - Loss: 0.2234\n","Batch 210/1952 - Loss: 0.1975\n","Batch 220/1952 - Loss: 0.4380\n","Batch 230/1952 - Loss: 0.3588\n","Batch 240/1952 - Loss: 0.2194\n","Batch 250/1952 - Loss: 0.2985\n","Batch 260/1952 - Loss: 0.3042\n","Batch 270/1952 - Loss: 0.2807\n","Batch 280/1952 - Loss: 0.1927\n","Batch 290/1952 - Loss: 0.2697\n","Batch 300/1952 - Loss: 0.1873\n","Batch 310/1952 - Loss: 0.2486\n","Batch 320/1952 - Loss: 0.3420\n","Batch 330/1952 - Loss: 0.2617\n","Batch 340/1952 - Loss: 0.3527\n","Batch 350/1952 - Loss: 0.2358\n","Batch 360/1952 - Loss: 0.3202\n","Batch 370/1952 - Loss: 0.2165\n","Batch 380/1952 - Loss: 0.1621\n","Batch 390/1952 - Loss: 0.3311\n","Batch 400/1952 - Loss: 0.3223\n","Batch 410/1952 - Loss: 0.2683\n","Batch 420/1952 - Loss: 0.3016\n","Batch 430/1952 - Loss: 0.2678\n","Batch 440/1952 - Loss: 0.2562\n","Batch 450/1952 - Loss: 0.3009\n","Batch 460/1952 - Loss: 0.2294\n","Batch 470/1952 - Loss: 0.3691\n","Batch 480/1952 - Loss: 0.2702\n","Batch 490/1952 - Loss: 0.2380\n","Batch 500/1952 - Loss: 0.2857\n","Batch 510/1952 - Loss: 0.1624\n","Batch 520/1952 - Loss: 0.2462\n","Batch 530/1952 - Loss: 0.3952\n","Batch 540/1952 - Loss: 0.2117\n","Batch 550/1952 - Loss: 0.3381\n","Batch 560/1952 - Loss: 0.4067\n","Batch 570/1952 - Loss: 0.2814\n","Batch 580/1952 - Loss: 0.2405\n","Batch 590/1952 - Loss: 0.1770\n","Batch 600/1952 - Loss: 0.2700\n","Batch 610/1952 - Loss: 0.2353\n","Batch 620/1952 - Loss: 0.1496\n","Batch 630/1952 - Loss: 0.2811\n","Batch 640/1952 - Loss: 0.1792\n","Batch 650/1952 - Loss: 0.2988\n","Batch 660/1952 - Loss: 0.2173\n","Batch 670/1952 - Loss: 0.2660\n","Batch 680/1952 - Loss: 0.2718\n","Batch 690/1952 - Loss: 0.1847\n","Batch 700/1952 - Loss: 0.2017\n","Batch 710/1952 - Loss: 0.3764\n","Batch 720/1952 - Loss: 0.1585\n","Batch 730/1952 - Loss: 0.1877\n","Batch 740/1952 - Loss: 0.1640\n","Batch 750/1952 - Loss: 0.2271\n","Batch 760/1952 - Loss: 0.2700\n","Batch 770/1952 - Loss: 0.2012\n","Batch 780/1952 - Loss: 0.2470\n","Batch 790/1952 - Loss: 0.2280\n","Batch 800/1952 - Loss: 0.1975\n","Batch 810/1952 - Loss: 0.3360\n","Batch 820/1952 - Loss: 0.3218\n","Batch 830/1952 - Loss: 0.2089\n","Batch 840/1952 - Loss: 0.1802\n","Batch 850/1952 - Loss: 0.2653\n","Batch 860/1952 - Loss: 0.2172\n","Batch 870/1952 - Loss: 0.1821\n","Batch 880/1952 - Loss: 0.1624\n","Batch 890/1952 - Loss: 0.3002\n","Batch 900/1952 - Loss: 0.2537\n","Batch 910/1952 - Loss: 0.3319\n","Batch 920/1952 - Loss: 0.1941\n","Batch 930/1952 - Loss: 0.1847\n","Batch 940/1952 - Loss: 0.1852\n","Batch 950/1952 - Loss: 0.2949\n","Batch 960/1952 - Loss: 0.2191\n","Batch 970/1952 - Loss: 0.2788\n","Batch 980/1952 - Loss: 0.1350\n","Batch 990/1952 - Loss: 0.2709\n","Batch 1000/1952 - Loss: 0.2708\n","Batch 1010/1952 - Loss: 0.2804\n","Batch 1020/1952 - Loss: 0.1101\n","Batch 1030/1952 - Loss: 0.1942\n","Batch 1040/1952 - Loss: 0.2621\n","Batch 1050/1952 - Loss: 0.2455\n","Batch 1060/1952 - Loss: 0.1390\n","Batch 1070/1952 - Loss: 0.3004\n","Batch 1080/1952 - Loss: 0.2425\n","Batch 1090/1952 - Loss: 0.2508\n","Batch 1100/1952 - Loss: 0.3136\n","Batch 1110/1952 - Loss: 0.2773\n","Batch 1120/1952 - Loss: 0.2251\n","Batch 1130/1952 - Loss: 0.3012\n","Batch 1140/1952 - Loss: 0.1706\n","Batch 1150/1952 - Loss: 0.1706\n","Batch 1160/1952 - Loss: 0.2988\n","Batch 1170/1952 - Loss: 0.2672\n","Batch 1180/1952 - Loss: 0.2005\n","Batch 1190/1952 - Loss: 0.2197\n","Batch 1200/1952 - Loss: 0.2574\n","Batch 1210/1952 - Loss: 0.1843\n","Batch 1220/1952 - Loss: 0.1789\n","Batch 1230/1952 - Loss: 0.2537\n","Batch 1240/1952 - Loss: 0.2631\n","Batch 1250/1952 - Loss: 0.2556\n","Batch 1260/1952 - Loss: 0.1329\n","Batch 1270/1952 - Loss: 0.1257\n","Batch 1280/1952 - Loss: 0.2708\n","Batch 1290/1952 - Loss: 0.2327\n","Batch 1300/1952 - Loss: 0.2655\n","Batch 1310/1952 - Loss: 0.2092\n","Batch 1320/1952 - Loss: 0.1447\n","Batch 1330/1952 - Loss: 0.2769\n","Batch 1340/1952 - Loss: 0.2541\n","Batch 1350/1952 - Loss: 0.0973\n","Batch 1360/1952 - Loss: 0.1297\n","Batch 1370/1952 - Loss: 0.1794\n","Batch 1380/1952 - Loss: 0.2241\n","Batch 1390/1952 - Loss: 0.0965\n","Batch 1400/1952 - Loss: 0.2670\n","Batch 1410/1952 - Loss: 0.2270\n","Batch 1420/1952 - Loss: 0.2873\n","Batch 1430/1952 - Loss: 0.2000\n","Batch 1440/1952 - Loss: 0.1587\n","Batch 1450/1952 - Loss: 0.2361\n","Batch 1460/1952 - Loss: 0.2183\n","Batch 1470/1952 - Loss: 0.2446\n","Batch 1480/1952 - Loss: 0.3016\n","Batch 1490/1952 - Loss: 0.1953\n","Batch 1500/1952 - Loss: 0.2613\n","Batch 1510/1952 - Loss: 0.0851\n","Batch 1520/1952 - Loss: 0.2256\n","Batch 1530/1952 - Loss: 0.1521\n","Batch 1540/1952 - Loss: 0.4209\n","Batch 1550/1952 - Loss: 0.1667\n","Batch 1560/1952 - Loss: 0.1439\n","Batch 1570/1952 - Loss: 0.2653\n","Batch 1580/1952 - Loss: 0.2500\n","Batch 1590/1952 - Loss: 0.2668\n","Batch 1600/1952 - Loss: 0.3052\n","Batch 1610/1952 - Loss: 0.2068\n","Batch 1620/1952 - Loss: 0.2703\n","Batch 1630/1952 - Loss: 0.3040\n","Batch 1640/1952 - Loss: 0.2238\n","Batch 1650/1952 - Loss: 0.1862\n","Batch 1660/1952 - Loss: 0.2294\n","Batch 1670/1952 - Loss: 0.2700\n","Batch 1680/1952 - Loss: 0.1246\n","Batch 1690/1952 - Loss: 0.1683\n","Batch 1700/1952 - Loss: 0.2375\n","Batch 1710/1952 - Loss: 0.1440\n","Batch 1720/1952 - Loss: 0.2373\n","Batch 1730/1952 - Loss: 0.1472\n","Batch 1740/1952 - Loss: 0.2844\n","Batch 1750/1952 - Loss: 0.1974\n","Batch 1760/1952 - Loss: 0.1383\n","Batch 1770/1952 - Loss: 0.2153\n","Batch 1780/1952 - Loss: 0.2631\n","Batch 1790/1952 - Loss: 0.1890\n","Batch 1800/1952 - Loss: 0.3292\n","Batch 1810/1952 - Loss: 0.1412\n","Batch 1820/1952 - Loss: 0.3149\n","Batch 1830/1952 - Loss: 0.1470\n","Batch 1840/1952 - Loss: 0.1669\n","Batch 1850/1952 - Loss: 0.2489\n","Batch 1860/1952 - Loss: 0.2150\n","Batch 1870/1952 - Loss: 0.1805\n","Batch 1880/1952 - Loss: 0.1987\n","Batch 1890/1952 - Loss: 0.2173\n","Batch 1900/1952 - Loss: 0.1725\n","Batch 1910/1952 - Loss: 0.2078\n","Batch 1920/1952 - Loss: 0.1576\n","Batch 1930/1952 - Loss: 0.1382\n","Batch 1940/1952 - Loss: 0.1949\n","Batch 1950/1952 - Loss: 0.2301\n","Epoch 2/100, Loss: 0.2319\n","Epoch 3/100 started\n","Batch 0/1952 - Loss: 0.1290\n","Batch 10/1952 - Loss: 0.2350\n","Batch 20/1952 - Loss: 0.2275\n","Batch 30/1952 - Loss: 0.2665\n","Batch 40/1952 - Loss: 0.1338\n","Batch 50/1952 - Loss: 0.1992\n","Batch 60/1952 - Loss: 0.1642\n","Batch 70/1952 - Loss: 0.2160\n","Batch 80/1952 - Loss: 0.2087\n","Batch 90/1952 - Loss: 0.2126\n","Batch 100/1952 - Loss: 0.2005\n","Batch 110/1952 - Loss: 0.2392\n","Batch 120/1952 - Loss: 0.2305\n","Batch 130/1952 - Loss: 0.2674\n","Batch 140/1952 - Loss: 0.2981\n","Batch 150/1952 - Loss: 0.1269\n","Batch 160/1952 - Loss: 0.3622\n","Batch 170/1952 - Loss: 0.2195\n","Batch 180/1952 - Loss: 0.1000\n","Batch 190/1952 - Loss: 0.1406\n","Batch 200/1952 - Loss: 0.1299\n","Batch 210/1952 - Loss: 0.2378\n","Batch 220/1952 - Loss: 0.2208\n","Batch 230/1952 - Loss: 0.1877\n","Batch 240/1952 - Loss: 0.2997\n","Batch 250/1952 - Loss: 0.1735\n","Batch 260/1952 - Loss: 0.2600\n","Batch 270/1952 - Loss: 0.1483\n","Batch 280/1952 - Loss: 0.0888\n","Batch 290/1952 - Loss: 0.2071\n","Batch 300/1952 - Loss: 0.1852\n","Batch 310/1952 - Loss: 0.2221\n","Batch 320/1952 - Loss: 0.1729\n","Batch 330/1952 - Loss: 0.3191\n","Batch 340/1952 - Loss: 0.2381\n","Batch 350/1952 - Loss: 0.1222\n","Batch 360/1952 - Loss: 0.0936\n","Batch 370/1952 - Loss: 0.3163\n","Batch 380/1952 - Loss: 0.1658\n","Batch 390/1952 - Loss: 0.1602\n","Batch 400/1952 - Loss: 0.3210\n","Batch 410/1952 - Loss: 0.1455\n","Batch 420/1952 - Loss: 0.1021\n","Batch 430/1952 - Loss: 0.0677\n","Batch 440/1952 - Loss: 0.1637\n","Batch 450/1952 - Loss: 0.1756\n","Batch 460/1952 - Loss: 0.2101\n","Batch 470/1952 - Loss: 0.2728\n","Batch 480/1952 - Loss: 0.1098\n","Batch 490/1952 - Loss: 0.0767\n","Batch 500/1952 - Loss: 0.1373\n","Batch 510/1952 - Loss: 0.2082\n","Batch 520/1952 - Loss: 0.2292\n","Batch 530/1952 - Loss: 0.2053\n","Batch 540/1952 - Loss: 0.2650\n","Batch 550/1952 - Loss: 0.1716\n","Batch 560/1952 - Loss: 0.2238\n","Batch 570/1952 - Loss: 0.1432\n","Batch 580/1952 - Loss: 0.1409\n","Batch 590/1952 - Loss: 0.1434\n","Batch 600/1952 - Loss: 0.1382\n","Batch 610/1952 - Loss: 0.2330\n","Batch 620/1952 - Loss: 0.2494\n","Batch 630/1952 - Loss: 0.0931\n","Batch 640/1952 - Loss: 0.2453\n","Batch 650/1952 - Loss: 0.2461\n","Batch 660/1952 - Loss: 0.1216\n","Batch 670/1952 - Loss: 0.2551\n","Batch 680/1952 - Loss: 0.1081\n","Batch 690/1952 - Loss: 0.1474\n","Batch 700/1952 - Loss: 0.1625\n","Batch 710/1952 - Loss: 0.3164\n","Batch 720/1952 - Loss: 0.2097\n","Batch 730/1952 - Loss: 0.2705\n","Batch 740/1952 - Loss: 0.1101\n","Batch 750/1952 - Loss: 0.1460\n","Batch 760/1952 - Loss: 0.2539\n","Batch 770/1952 - Loss: 0.0642\n","Batch 780/1952 - Loss: 0.2627\n","Batch 790/1952 - Loss: 0.1757\n","Batch 800/1952 - Loss: 0.1161\n","Batch 810/1952 - Loss: 0.2419\n","Batch 820/1952 - Loss: 0.1491\n","Batch 830/1952 - Loss: 0.3681\n","Batch 840/1952 - Loss: 0.1590\n","Batch 850/1952 - Loss: 0.2662\n","Batch 860/1952 - Loss: 0.3643\n","Batch 870/1952 - Loss: 0.2784\n","Batch 880/1952 - Loss: 0.1679\n","Batch 890/1952 - Loss: 0.2820\n","Batch 900/1952 - Loss: 0.1515\n","Batch 910/1952 - Loss: 0.0604\n","Batch 920/1952 - Loss: 0.0998\n","Batch 930/1952 - Loss: 0.1556\n","Batch 940/1952 - Loss: 0.2335\n","Batch 950/1952 - Loss: 0.2586\n","Batch 960/1952 - Loss: 0.2465\n","Batch 970/1952 - Loss: 0.2131\n","Batch 980/1952 - Loss: 0.2125\n","Batch 990/1952 - Loss: 0.2177\n","Batch 1000/1952 - Loss: 0.2315\n","Batch 1010/1952 - Loss: 0.1851\n","Batch 1020/1952 - Loss: 0.1134\n","Batch 1030/1952 - Loss: 0.2066\n","Batch 1040/1952 - Loss: 0.1233\n","Batch 1050/1952 - Loss: 0.2299\n","Batch 1060/1952 - Loss: 0.1648\n","Batch 1070/1952 - Loss: 0.2827\n","Batch 1080/1952 - Loss: 0.1330\n","Batch 1090/1952 - Loss: 0.1563\n","Batch 1100/1952 - Loss: 0.1909\n","Batch 1110/1952 - Loss: 0.2211\n","Batch 1120/1952 - Loss: 0.1497\n","Batch 1130/1952 - Loss: 0.2307\n","Batch 1140/1952 - Loss: 0.1955\n","Batch 1150/1952 - Loss: 0.2524\n","Batch 1160/1952 - Loss: 0.2488\n","Batch 1170/1952 - Loss: 0.2021\n","Batch 1180/1952 - Loss: 0.2285\n","Batch 1190/1952 - Loss: 0.1766\n","Batch 1200/1952 - Loss: 0.1595\n","Batch 1210/1952 - Loss: 0.1373\n","Batch 1220/1952 - Loss: 0.1791\n","Batch 1230/1952 - Loss: 0.1849\n","Batch 1240/1952 - Loss: 0.1038\n","Batch 1250/1952 - Loss: 0.2353\n","Batch 1260/1952 - Loss: 0.2133\n","Batch 1270/1952 - Loss: 0.1066\n","Batch 1280/1952 - Loss: 0.1476\n","Batch 1290/1952 - Loss: 0.1895\n","Batch 1300/1952 - Loss: 0.1382\n","Batch 1310/1952 - Loss: 0.1606\n","Batch 1320/1952 - Loss: 0.1855\n","Batch 1330/1952 - Loss: 0.3064\n","Batch 1340/1952 - Loss: 0.2191\n","Batch 1350/1952 - Loss: 0.2116\n","Batch 1360/1952 - Loss: 0.2145\n","Batch 1370/1952 - Loss: 0.1639\n","Batch 1380/1952 - Loss: 0.1405\n","Batch 1390/1952 - Loss: 0.1613\n","Batch 1400/1952 - Loss: 0.2502\n","Batch 1410/1952 - Loss: 0.0914\n","Batch 1420/1952 - Loss: 0.1580\n","Batch 1430/1952 - Loss: 0.2165\n","Batch 1440/1952 - Loss: 0.2690\n","Batch 1450/1952 - Loss: 0.2663\n","Batch 1460/1952 - Loss: 0.1398\n","Batch 1470/1952 - Loss: 0.1844\n","Batch 1480/1952 - Loss: 0.1850\n","Batch 1490/1952 - Loss: 0.2586\n","Batch 1500/1952 - Loss: 0.1336\n","Batch 1510/1952 - Loss: 0.1987\n","Batch 1520/1952 - Loss: 0.0767\n","Batch 1530/1952 - Loss: 0.1293\n","Batch 1540/1952 - Loss: 0.2003\n","Batch 1550/1952 - Loss: 0.0992\n","Batch 1560/1952 - Loss: 0.2981\n","Batch 1570/1952 - Loss: 0.2550\n","Batch 1580/1952 - Loss: 0.1195\n","Batch 1590/1952 - Loss: 0.1319\n","Batch 1600/1952 - Loss: 0.3234\n","Batch 1610/1952 - Loss: 0.1729\n","Batch 1620/1952 - Loss: 0.2587\n","Batch 1630/1952 - Loss: 0.1656\n","Batch 1640/1952 - Loss: 0.1367\n","Batch 1650/1952 - Loss: 0.1651\n","Batch 1660/1952 - Loss: 0.3063\n","Batch 1670/1952 - Loss: 0.2149\n","Batch 1680/1952 - Loss: 0.1739\n","Batch 1690/1952 - Loss: 0.1964\n","Batch 1700/1952 - Loss: 0.1100\n","Batch 1710/1952 - Loss: 0.1997\n","Batch 1720/1952 - Loss: 0.2023\n","Batch 1730/1952 - Loss: 0.1622\n","Batch 1740/1952 - Loss: 0.0795\n","Batch 1750/1952 - Loss: 0.1368\n","Batch 1760/1952 - Loss: 0.1099\n","Batch 1770/1952 - Loss: 0.1403\n","Batch 1780/1952 - Loss: 0.2504\n","Batch 1790/1952 - Loss: 0.0454\n","Batch 1800/1952 - Loss: 0.0893\n","Batch 1810/1952 - Loss: 0.1229\n","Batch 1820/1952 - Loss: 0.2231\n","Batch 1830/1952 - Loss: 0.2637\n","Batch 1840/1952 - Loss: 0.2140\n","Batch 1850/1952 - Loss: 0.0550\n","Batch 1860/1952 - Loss: 0.1685\n","Batch 1870/1952 - Loss: 0.2475\n","Batch 1880/1952 - Loss: 0.1658\n","Batch 1890/1952 - Loss: 0.1363\n","Batch 1900/1952 - Loss: 0.1318\n","Batch 1910/1952 - Loss: 0.1577\n","Batch 1920/1952 - Loss: 0.1047\n","Batch 1930/1952 - Loss: 0.1724\n","Batch 1940/1952 - Loss: 0.0519\n","Batch 1950/1952 - Loss: 0.3117\n","Epoch 3/100, Loss: 0.1849\n","Epoch 4/100 started\n","Batch 0/1952 - Loss: 0.0846\n","Batch 10/1952 - Loss: 0.1706\n","Batch 20/1952 - Loss: 0.2394\n","Batch 30/1952 - Loss: 0.1428\n","Batch 40/1952 - Loss: 0.0642\n","Batch 50/1952 - Loss: 0.2135\n","Batch 60/1952 - Loss: 0.1118\n","Batch 70/1952 - Loss: 0.1960\n","Batch 80/1952 - Loss: 0.1041\n","Batch 90/1952 - Loss: 0.0899\n","Batch 100/1952 - Loss: 0.1188\n","Batch 110/1952 - Loss: 0.1415\n","Batch 120/1952 - Loss: 0.1803\n","Batch 130/1952 - Loss: 0.0955\n","Batch 140/1952 - Loss: 0.1663\n","Batch 150/1952 - Loss: 0.1633\n","Batch 160/1952 - Loss: 0.1003\n","Batch 170/1952 - Loss: 0.1952\n","Batch 180/1952 - Loss: 0.1324\n","Batch 190/1952 - Loss: 0.1168\n","Batch 200/1952 - Loss: 0.2587\n","Batch 210/1952 - Loss: 0.2108\n","Batch 220/1952 - Loss: 0.2230\n","Batch 230/1952 - Loss: 0.1688\n","Batch 240/1952 - Loss: 0.1075\n","Batch 250/1952 - Loss: 0.1411\n","Batch 260/1952 - Loss: 0.0812\n","Batch 270/1952 - Loss: 0.1015\n","Batch 280/1952 - Loss: 0.0722\n","Batch 290/1952 - Loss: 0.0685\n","Batch 300/1952 - Loss: 0.1059\n","Batch 310/1952 - Loss: 0.1440\n","Batch 320/1952 - Loss: 0.1625\n","Batch 330/1952 - Loss: 0.2632\n","Batch 340/1952 - Loss: 0.1497\n","Batch 350/1952 - Loss: 0.1478\n","Batch 360/1952 - Loss: 0.1495\n","Batch 370/1952 - Loss: 0.1777\n","Batch 380/1952 - Loss: 0.1275\n","Batch 390/1952 - Loss: 0.1565\n","Batch 400/1952 - Loss: 0.1512\n","Batch 410/1952 - Loss: 0.2295\n","Batch 420/1952 - Loss: 0.1088\n","Batch 430/1952 - Loss: 0.2074\n","Batch 440/1952 - Loss: 0.2302\n","Batch 450/1952 - Loss: 0.0934\n","Batch 460/1952 - Loss: 0.1238\n","Batch 470/1952 - Loss: 0.2274\n","Batch 480/1952 - Loss: 0.0927\n","Batch 490/1952 - Loss: 0.1962\n","Batch 500/1952 - Loss: 0.2326\n","Batch 510/1952 - Loss: 0.1147\n","Batch 520/1952 - Loss: 0.0237\n","Batch 530/1952 - Loss: 0.1419\n","Batch 540/1952 - Loss: 0.0675\n","Batch 550/1952 - Loss: 0.1857\n","Batch 560/1952 - Loss: 0.0780\n","Batch 570/1952 - Loss: 0.2015\n","Batch 580/1952 - Loss: 0.0450\n","Batch 590/1952 - Loss: 0.1646\n","Batch 600/1952 - Loss: 0.2545\n","Batch 610/1952 - Loss: 0.0977\n","Batch 620/1952 - Loss: 0.1557\n","Batch 630/1952 - Loss: 0.0675\n","Batch 640/1952 - Loss: 0.0686\n","Batch 650/1952 - Loss: 0.1508\n","Batch 660/1952 - Loss: 0.0744\n","Batch 670/1952 - Loss: 0.1685\n","Batch 680/1952 - Loss: 0.2134\n","Batch 690/1952 - Loss: 0.1425\n","Batch 700/1952 - Loss: 0.0754\n","Batch 710/1952 - Loss: 0.1420\n","Batch 720/1952 - Loss: 0.2074\n","Batch 730/1952 - Loss: 0.1466\n","Batch 740/1952 - Loss: 0.1040\n","Batch 750/1952 - Loss: 0.0736\n","Batch 760/1952 - Loss: 0.1464\n","Batch 770/1952 - Loss: 0.1573\n","Batch 780/1952 - Loss: 0.1778\n","Batch 790/1952 - Loss: 0.1833\n","Batch 800/1952 - Loss: 0.1114\n","Batch 810/1952 - Loss: 0.1587\n","Batch 820/1952 - Loss: 0.1212\n","Batch 830/1952 - Loss: 0.1043\n","Batch 840/1952 - Loss: 0.1191\n","Batch 850/1952 - Loss: 0.1564\n","Batch 860/1952 - Loss: 0.0632\n","Batch 870/1952 - Loss: 0.1426\n","Batch 880/1952 - Loss: 0.2777\n","Batch 890/1952 - Loss: 0.0913\n","Batch 900/1952 - Loss: 0.1000\n","Batch 910/1952 - Loss: 0.2232\n","Batch 920/1952 - Loss: 0.1451\n","Batch 930/1952 - Loss: 0.1222\n","Batch 940/1952 - Loss: 0.1555\n","Batch 950/1952 - Loss: 0.1402\n","Batch 960/1952 - Loss: 0.1289\n","Batch 970/1952 - Loss: 0.1065\n","Batch 980/1952 - Loss: 0.1686\n","Batch 990/1952 - Loss: 0.2195\n","Batch 1000/1952 - Loss: 0.0963\n"]}]}]}