{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11200684,"sourceType":"datasetVersion","datasetId":6993307}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-28T22:47:52.880987Z","iopub.execute_input":"2025-03-28T22:47:52.881394Z","iopub.status.idle":"2025-03-28T22:47:52.893103Z","shell.execute_reply.started":"2025-03-28T22:47:52.881360Z","shell.execute_reply":"2025-03-28T22:47:52.891820Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/new-dataset/combined_augmented_dataset.csv\n/kaggle/input/new-dataset/test_df.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install transformers torch pandas scikit-learn numpy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T22:49:04.558296Z","iopub.execute_input":"2025-03-28T22:49:04.558700Z","iopub.status.idle":"2025-03-28T22:49:11.376255Z","shell.execute_reply.started":"2025-03-28T22:49:04.558662Z","shell.execute_reply":"2025-03-28T22:49:11.375116Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModel, AdamW\nimport numpy as np\nimport ast","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T06:06:14.028445Z","iopub.execute_input":"2025-03-29T06:06:14.028778Z","iopub.status.idle":"2025-03-29T06:06:14.032891Z","shell.execute_reply.started":"2025-03-29T06:06:14.028750Z","shell.execute_reply":"2025-03-29T06:06:14.031784Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Load the datasets\ntrain_df = pd.read_csv('/kaggle/input/new-dataset/combined_augmented_dataset.csv')\ntest_df = pd.read_csv('/kaggle/input/new-dataset/test_df.csv')\n\n# Preprocess techniques\ntrain_df['techniques'] = train_df['techniques'].apply(ast.literal_eval)\n\n# Define techniques columns\nall_techniques = ['straw_man', 'appeal_to_fear', 'fud', 'bandwagon',\n                  'whataboutism', 'loaded_language',\n                  'glittering_generalities', 'euphoria',\n                  'cherry_picking', 'cliche']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T06:06:16.949782Z","iopub.execute_input":"2025-03-29T06:06:16.950058Z","iopub.status.idle":"2025-03-29T06:06:17.508022Z","shell.execute_reply.started":"2025-03-29T06:06:16.950035Z","shell.execute_reply":"2025-03-29T06:06:17.507376Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Create label matrix for training data\nlabel_matrix = pd.DataFrame(0, index=train_df.index, columns=all_techniques)\nfor idx, techniques in enumerate(train_df['techniques']):\n    for technique in techniques:\n        if technique in all_techniques:\n            label_matrix.at[idx, technique] = 1\ntrain_df = pd.concat([train_df, label_matrix], axis=1)\n\n# Tokenization\nmodel_name = \"bert-base-multilingual-cased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ndef tokenize_data(df, tokenizer, max_length=512, is_test=False):\n    print(\"tokenize data\")\n    encodings = tokenizer(\n        df['content'].tolist(),\n        truncation=True,\n        padding=True,\n        max_length=max_length,\n        return_tensors='pt'\n    )\n    if is_test:\n        return encodings\n    labels = torch.tensor(df[all_techniques].values, dtype=torch.float)\n    return encodings, labels\n\n# Tokenize training and test data\ntrain_encodings, train_labels = tokenize_data(train_df, tokenizer)\ntest_encodings = tokenize_data(test_df, tokenizer, is_test=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T06:06:19.139196Z","iopub.execute_input":"2025-03-29T06:06:19.139524Z","iopub.status.idle":"2025-03-29T06:06:31.522557Z","shell.execute_reply.started":"2025-03-29T06:06:19.139493Z","shell.execute_reply":"2025-03-29T06:06:31.521614Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c76e057a0ec4d9ba6039dc402b0079a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9807c75dc38c486d9dea2fbab32c2ae5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a8a2d9a21414f08a77067ae790a842b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8996fec7fbb14dc38a5fc3972af4efc3"}},"metadata":{}},{"name":"stdout","text":"tokenize data\ntokenize data\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Dataset Class\nclass TextDataset(Dataset):\n    print(\"in class dataset\")\n    def __init__(self, encodings, labels=None):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: val[idx] for key, val in self.encodings.items()}\n        if self.labels is not None:\n            item['labels'] = self.labels[idx]\n        return item\n\n    def __len__(self):\n        return len(self.encodings['input_ids'])\n\n# Create datasets\ntrain_dataset = TextDataset(train_encodings, train_labels)\ntest_dataset = TextDataset(test_encodings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T06:06:31.523901Z","iopub.execute_input":"2025-03-29T06:06:31.524214Z","iopub.status.idle":"2025-03-29T06:06:31.530546Z","shell.execute_reply.started":"2025-03-29T06:06:31.524180Z","shell.execute_reply":"2025-03-29T06:06:31.529685Z"}},"outputs":[{"name":"stdout","text":"in class dataset\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Model Definition\nclass MultiLabelClassifier(nn.Module):\n    def __init__(self, transformer_model_name, num_labels):\n        print(\"init\")\n        super(MultiLabelClassifier, self).__init__()\n        self.transformer = AutoModel.from_pretrained(transformer_model_name)\n        transformer_hidden_dim = self.transformer.config.hidden_size\n        self.dropout = nn.Dropout(0.3)\n        self.classifier = nn.Sequential(\n            nn.Linear(transformer_hidden_dim, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, num_labels)\n        )\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        transformer_outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = transformer_outputs.last_hidden_state[:, 0, :]\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        loss = None\n        if labels is not None:\n            loss_fn = nn.BCEWithLogitsLoss()\n            loss = loss_fn(logits, labels)\n        return loss, logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T06:06:32.810312Z","iopub.execute_input":"2025-03-29T06:06:32.810668Z","iopub.status.idle":"2025-03-29T06:06:32.816610Z","shell.execute_reply.started":"2025-03-29T06:06:32.810640Z","shell.execute_reply":"2025-03-29T06:06:32.815620Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Training Setup\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = MultiLabelClassifier(\n    transformer_model_name=model_name,\n    num_labels=len(all_techniques)\n)\nmodel.to(device)\n\n# Optimizer and Learning Rate\noptimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n\n# Training Parameters\nbatch_size = 8\nnum_epochs = 100  # Changed to 100 epochs\npatience = 2  # Number of epochs to wait for improvement\nbest_loss = float('inf')\npatience_counter = 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T06:06:34.979759Z","iopub.execute_input":"2025-03-29T06:06:34.980069Z","iopub.status.idle":"2025-03-29T06:06:50.661926Z","shell.execute_reply.started":"2025-03-29T06:06:34.980040Z","shell.execute_reply":"2025-03-29T06:06:50.660993Z"}},"outputs":[{"name":"stdout","text":"init\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce0b14d48f0145a9b46f4056851cbe53"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T06:06:50.663181Z","iopub.execute_input":"2025-03-29T06:06:50.663849Z","iopub.status.idle":"2025-03-29T06:06:50.668007Z","shell.execute_reply.started":"2025-03-29T06:06:50.663821Z","shell.execute_reply":"2025-03-29T06:06:50.667058Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Training Loop with Early Stopping\nfor epoch in range(num_epochs):\n    print(\"epoch started\")\n    model.train()\n    total_loss = 0\n\n    for batch in train_loader:\n        optimizer.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        loss, logits = model(input_ids, attention_mask, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    # Calculate average loss\n    avg_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n\n    # Early stopping check\n    if avg_loss < best_loss:\n        best_loss = avg_loss\n        patience_counter = 0\n        # Optional: Save the best model\n        torch.save(model.state_dict(), 'best_model.pt')\n    else:\n        patience_counter += 1\n        print(f\"Patience counter: {patience_counter}/{patience}\")\n        \n    if patience_counter >= patience:\n        print(f\"Early stopping triggered after epoch {epoch+1}\")\n        break\n\n# Load best model for prediction\nmodel.load_state_dict(torch.load('best_model.pt'))\nmodel.eval()\npredictions = []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        _, logits = model(input_ids, attention_mask)\n        probs = torch.sigmoid(logits)\n        preds = (probs > 0.5).float()\n        predictions.extend(preds.cpu().numpy())\n\n# Create Submission DataFrame\nsubmission_df = pd.DataFrame(columns=['id'] + all_techniques)\nsubmission_df['id'] = test_df['id']\n\nfor technique in all_techniques:\n    submission_df[technique] = [pred[all_techniques.index(technique)] for pred in predictions]\n\n# Save Submission\nsubmission_df.to_csv('submission.csv', index=False)\nprint(\"Submission file created successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T06:07:07.340715Z","iopub.execute_input":"2025-03-29T06:07:07.340996Z","iopub.status.idle":"2025-03-29T12:21:48.243371Z","shell.execute_reply.started":"2025-03-29T06:07:07.340974Z","shell.execute_reply":"2025-03-29T12:21:48.242656Z"}},"outputs":[{"name":"stdout","text":"epoch started\nEpoch 1/100, Loss: 0.2858\nepoch started\nEpoch 2/100, Loss: 0.2025\nepoch started\nEpoch 3/100, Loss: 0.1322\nepoch started\nEpoch 4/100, Loss: 0.0805\nepoch started\nEpoch 5/100, Loss: 0.0509\nepoch started\nEpoch 6/100, Loss: 0.0355\nepoch started\nEpoch 7/100, Loss: 0.0282\nepoch started\nEpoch 8/100, Loss: 0.0240\nepoch started\nEpoch 9/100, Loss: 0.0219\nepoch started\nEpoch 10/100, Loss: 0.0194\nepoch started\nEpoch 11/100, Loss: 0.0182\nepoch started\nEpoch 12/100, Loss: 0.0160\nepoch started\nEpoch 13/100, Loss: 0.0164\nPatience counter: 1/2\nepoch started\nEpoch 14/100, Loss: 0.0141\nepoch started\nEpoch 15/100, Loss: 0.0142\nPatience counter: 1/2\nepoch started\nEpoch 16/100, Loss: 0.0113\nepoch started\nEpoch 17/100, Loss: 0.0135\nPatience counter: 1/2\nepoch started\nEpoch 18/100, Loss: 0.0122\nPatience counter: 2/2\nEarly stopping triggered after epoch 18\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-9-9b698800250e>:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best_model.pt'))\n","output_type":"stream"},{"name":"stdout","text":"Submission file created successfully!\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}