{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1aye_2nDLmRZpUGaPKmlAu1gK1bKX9G0J","authorship_tag":"ABX9TyPWnP0tCkzsG3/8lZylYE4P"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import torch\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim import AdamW\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","import numpy as np\n","import ast\n","from torch.cuda.amp import autocast, GradScaler"],"metadata":{"id":"i1lLqj2WkcXZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1) Load the datasets\n","train_df = pd.read_csv('/kaggle/input/processed-subtask1/subtask1-train_preprocessed.csv')\n","test_df = pd.read_csv('/kaggle/input/processed-subtask1/subtask1-test_preprocessed.csv')  # Corrected test data loading\n","\n","# 2) Preprocess the 'techniques' column\n","train_df['techniques'] = train_df['techniques'].apply(ast.literal_eval)"],"metadata":{"id":"gNx6OjcNkfGQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# 3) Define all technique labels\n","all_techniques = [\n","    'straw_man', 'appeal_to_fear', 'fud', 'bandwagon',\n","    'whataboutism', 'loaded_language',\n","    'glittering_generalities', 'euphoria',\n","    'cherry_picking', 'cliche'\n","]\n","\n","# 4) Build a multi-hot label matrix\n","label_matrix = pd.DataFrame(0, index=train_df.index, columns=all_techniques)\n","for idx, techniques in enumerate(train_df['techniques']):\n","    for tech in techniques:\n","        if tech in all_techniques:\n","            label_matrix.at[idx, tech] = 1\n","train_df = pd.concat([train_df, label_matrix], axis=1)\n","\n","# 5) Define technique descriptions\n","technique_descriptions = {tech: tech.replace('_', ' ') for tech in all_techniques}"],"metadata":{"id":"PphekA0MkiCH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 6) Load tokenizer and reranker model with XLM-RoBERTa Large\n","model_name = \"xlm-roberta-large\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","reranker_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n","\n","# 7) Set device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","reranker_model = reranker_model.to(device)\n","\n","# 8) Define optimizer and loss function\n","optimizer = AdamW(reranker_model.parameters(), lr=2e-5, weight_decay=0.01)\n","loss_fn = nn.BCEWithLogitsLoss()\n","scaler = GradScaler()"],"metadata":{"id":"-zWavzs3knHw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 9) Dataset wrapper\n","class TextDataset(Dataset):\n","    def __init__(self, df, has_labels=True):\n","        self.df = df\n","        self.has_labels = has_labels\n","\n","    def __getitem__(self, idx):\n","        item = {'content': self.df.iloc[idx]['content']}\n","        if self.has_labels:\n","            labels = self.df.iloc[idx][all_techniques].values.astype(float)\n","            item['labels'] = torch.tensor(labels, dtype=torch.float)\n","        return item\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","# 10) Create datasets and dataloaders\n","train_dataset = TextDataset(train_df, has_labels=True)\n","test_dataset = TextDataset(test_df, has_labels=False)\n","batch_size = 8\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, pin_memory=True)\n"],"metadata":{"id":"1fR2qjpzkqMn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 11) Training loop with early stopping\n","num_epochs = 100  # Adjusted to 100 as per user preference\n","patience = 4\n","best_loss = float('inf')\n","patience_counter = 0\n","accumulation_steps = 2\n","\n","for epoch in range(num_epochs):\n","    reranker_model.train()\n","    total_loss = 0.0\n","    optimizer.zero_grad()\n","\n","    for i, batch in enumerate(train_loader):\n","        batch_texts = batch['content']\n","        batch_labels = batch['labels'].to(device)  # (B, N)\n","\n","        B = len(batch_texts)\n","        N = len(all_techniques)\n","        queries = [technique_descriptions[tech] for tech in all_techniques for _ in range(B)]\n","        passages = [text for _ in range(N) for text in batch_texts]\n","\n","        # Tokenize\n","        inputs = tokenizer(queries, passages, padding=True, truncation=True, return_tensors='pt', max_length=128)\n","        inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","        # Get scores\n","        with autocast():\n","            outputs = reranker_model(**inputs)\n","            scores = outputs.logits.view(B, N)  # (B, N)\n","            loss = loss_fn(scores, batch_labels) / accumulation_steps\n","\n","        scaler.scale(loss).backward()\n","\n","        if (i + 1) % accumulation_steps == 0:\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","\n","        total_loss += loss.item() * accumulation_steps\n","\n","        if i % 10 == 0:\n","            print(f\"Epoch {epoch+1}, Batch {i}, Loss {total_loss / (i+1):.4f}\")\n","\n","        # Free up memory\n","        del inputs, outputs, scores, loss\n","        torch.cuda.empty_cache()\n","\n","    avg_loss = total_loss / len(train_loader)\n","    print(f\"Epoch {epoch+1} completed â€” Avg Loss: {avg_loss:.4f}\")\n","\n","    if avg_loss < best_loss:\n","        best_loss = avg_loss\n","        patience_counter = 0\n","        torch.save(reranker_model.state_dict(), 'best_model.pt')\n","    else:\n","        patience_counter += 1\n","        if patience_counter >= patience:\n","            print(\"Early stopping triggered.\")\n","            break\n"],"metadata":{"id":"a5hhUpSpkxEW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# 12) Inference\n","reranker_model.load_state_dict(torch.load('best_model.pt'))\n","reranker_model.eval()\n","predictions = []\n","\n","with torch.no_grad():\n","    for batch in test_loader:\n","        batch_texts = batch['content']\n","\n","        B = len(batch_texts)\n","        N = len(all_techniques)\n","        queries = [technique_descriptions[tech] for tech in all_techniques for _ in range(B)]\n","        passages = [text for _ in range(N) for text in batch_texts]\n","\n","        inputs = tokenizer(queries, passages, padding=True, truncation=True, return_tensors='pt', max_length=128)\n","        inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","        with autocast():\n","            outputs = reranker_model(**inputs)\n","            scores = outputs.logits.view(B, N)\n","            probs = torch.sigmoid(scores)\n","            preds = (probs > 0.5).float()\n","            predictions.extend(preds.cpu().numpy())\n","\n","        del inputs, outputs, scores, probs, preds\n","        torch.cuda.empty_cache()\n","\n","# 13) Build submission\n","submission_df = pd.DataFrame(columns=['id'] + all_techniques)\n","submission_df['id'] = test_df['id']\n","for idx, tech in enumerate(all_techniques):\n","    submission_df[tech] = [int(pred[idx]) for pred in predictions]\n","\n","submission_df.to_csv('submission.csv', index=False)\n","print(\"Submission file created successfully!\")"],"metadata":{"id":"wokZU-tHka9S"},"execution_count":null,"outputs":[]}]}